{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TSEI aims to provide **evaluation researchers** with convenient ways to:\n",
    "1. Create production-ready implementations of the evaluations/benchmarks they invent.\n",
    "2. Evaluate the functional and non-functional performance of their evaluations on leading open-source models.\n",
    "3. Compare their evaluations against others.\n",
    "4. Get quantifiable exposure and visibility.\n",
    "5. Get feedback.\n",
    "\n",
    "### High-Level User Journey\n",
    "Alice the student logs into the TSEI Safety Arena. She sees a UI that guides her through the process of creating a new evaluation, starting with the dataset she has. She can choose from different options for the task, the templates, the metrics, etc., then publish the new evaluation for review. The whole process requires no coding, and takes less than an hour.\n",
    "\n",
    "(the following day)\n",
    "\n",
    "Alice sees her new evaluation listed in the catalog, with computed scores following a nightly run. Some of the content is generated automatically. As owner, Alice can update/modify as needed.\n",
    "\n",
    ">For each evaluation, the catalog lists the following:\n",
    ">- name, owner, license and terms of use, description, link to a [Benchmark Card](https://arxiv.org/pdf/2410.12974)\n",
    ">- taxonomy tags\n",
    ">- size (in prompt count)\n",
    ">- BAT score, average refusal rate, average execution time\n",
    "\n",
    "(the following week)\n",
    "\n",
    "Alice receives a notification from TSEI about feedback left for her contributed evaluation, along with (periodic) report detailing stars/favorites, download count, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaderboard/Catalog of Evaluations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leaderboard view of the catalog will look something like this:\n",
    "\n",
    "| Name | Taxonomy Tags | Owner/License | Prompt Count | BAT Score | Avg. Refusal Rate | Avg. Execution Time |\n",
    "|---|---|---|---:|---:|---:|---:|\n",
    "| Hello | tag1,tag2,tag3 | World | 123 | 423 | 456 | 789 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Reference Evaluation Stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lm_eval[ibm_watsonx_ai]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Native lm-eval task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lm-eval --model watsonx_llm --model_args model_id=ibm/granite-3-8b-instruct --tasks babi --limit 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unitxt task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lm-eval --model watsonx_llm --model_args model_id=ibm/granite-3-8b-instruct --tasks ledgar --include_path unitxt --limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the Leaderboard\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
