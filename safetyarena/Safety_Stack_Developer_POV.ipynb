{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdcution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TSEI aims to provide **AI solution developers** with a convenient way to:\n",
    "1. Find and understand SOTA evaluations and benchmarks for AI safety (and other trustworthiness aspects).\n",
    "2. Identify the relevant subsets of evaluations that address the risks they are concerned about.\n",
    "3.  Download production-ready content they can integrate into their dev/rel environments.\n",
    "\n",
    "### High-Level User Journey\n",
    "Bob the builder opens the TSEI Safety Arena site, and sees a catalog of evaluations. He can filter the catalog by different dimensions, can search for specific properties, etc.\n",
    "\n",
    "For each evaluation, the catalog lists the following:\n",
    "- name, owner, license and terms of use, description, link to a [Benchmark Card](https://arxiv.org/pdf/2410.12974)\n",
    "- taxonomy tags\n",
    "- size (in prompt count)\n",
    "- BAT score, average refusal rate, average execution time\n",
    "\n",
    "Bob can select one or more evaluations he wants to use, then choose how to download them. For example:\n",
    "- packaged software bundle\n",
    "- Docker image with all the dependencies\n",
    "- Cloud-deployable container for IBM Cloud, AWS, OpenShift, etc.\n",
    "\n",
    "Bob can favorite/star evaluations, leave feedback.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaderboard/Catalog of Evaluations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leaderboard view of the catalog will look something like this:\n",
    "\n",
    "| Name | Taxonomy Tags | Owner/License | Prompt Count | BAT Score | Avg. Refusal Rate | Avg. Execution Time |\n",
    "|---|---|---|---:|---:|---:|---:|\n",
    "| Hello | tag1,tag2,tag3 | World | 123 | 423 | 456 | 789 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Reference Evaluation Stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lm_eval[ibm_watsonx_ai]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Native lm-eval task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lm-eval --model watsonx_llm --model_args model_id=ibm/granite-3-8b-instruct --tasks babi --limit 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unitxt task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lm-eval --model watsonx_llm --model_args model_id=ibm/granite-3-8b-instruct --tasks ledgar --include_path unitxt --limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the Leaderboard\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
